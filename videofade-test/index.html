<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Chat Bot Interaction</title>
<link rel="stylesheet" href="css/survey.css">
<style>
  html, body {
    background: black;
    /* keep room for fixed controls so they don't overlap the page content */
    padding-bottom: 88px;
  }
  /* Full-bleed backdrop behind the stage that fills the viewport width */
  #backdrop {
    position: fixed;
    left: 0;
    top: 0;
    width: 100vw;
    height: calc(100vh - 88px); /* leave room for controls */
    background: black;
    z-index: 0;
    pointer-events: none;
  }
  #stage { 
    position: relative; 
    width: 100%; 
    max-width: 960px; 
    aspect-ratio: 16/9; 
    background: transparent; /* allow backdrop to show through for blending */
    z-index: 10; /* sit above backdrop but below controls */
    margin: 0 auto;
    /* Add top margin to push stage down */
    margin-top: calc((100vh - 88px - (960px * 9/16)) / 2);
  }
  #stage video {
    position: absolute; inset: 0; width: 100%; height: 100%;
    object-fit: cover; opacity: 0; pointer-events: none; z-index: 1;
    border-radius: 12px;
    /* No CSS transition - crossfade is handled by JS Web Animations API */
  }
  #stage video.active { opacity: 1; }
  #stage video.on-top { z-index: 2; }

  /* Black box overlay in bottom right corner */
  #blackBox {
    position: absolute;
    bottom: 0;
    right: 0;
    width: 20%;
    height: 20%;
    background: #000;
    z-index: 10;
    pointer-events: none;
    display: none; /* Hidden - not needed for videos with backgrounds */
  }

  /* Fullscreen styles */
  #stage:fullscreen {
    max-width: 100%;
    width: 100vw;
    height: 100vh;
    margin: 0;
  }
  #stage:-webkit-full-screen {
    max-width: 100%;
    width: 100vw;
    height: 100vh;
    margin: 0;
  }
  #stage:-moz-full-screen {
    max-width: 100%;
    width: 100vw;
    height: 100vh;
    margin: 0;
  }
  /* Controls are fixed to the bottom of the viewport (floating bar) */
  #controls {
    position: fixed;
    left: 0;
    right: 0;
    bottom: 0;
    display: flex;
    gap: .75rem;
    align-items: center;
    justify-content: center;
    padding: 10px 14px;
    background: rgba(0,0,0,0.55);
    backdrop-filter: blur(6px);
    z-index: 50;
    flex-wrap: wrap;
    font: 14px/1.4 system-ui, sans-serif;
    color: #ddd;
  }
  button { padding:.45rem .8rem; border:1px solid #444; background:#111; color:#eee; border-radius:.5rem; cursor:pointer; }
  button:hover { background:#222; }
  input[type="range"] { width: 260px; }
  .val { min-width: 1.5ch; text-align: right; display:inline-block; }

  /* Voice Agent Button */
  #voiceAgentBtn {
    padding: 0.6rem 1.2rem;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    border: 2px solid #8b5cf6;
    color: white;
    font-weight: 600;
    font-size: 15px;
    transition: all 0.3s ease;
    position: relative;
    overflow: hidden;
  }
  #voiceAgentBtn:hover {
    background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(139, 92, 246, 0.4);
  }
  #voiceAgentBtn.active {
    background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
    border-color: #ef4444;
    animation: pulse 1.5s infinite;
  }
  #voiceAgentBtn.connected {
    background: linear-gradient(135deg, #10b981 0%, #059669 100%);
    border-color: #10b981;
  }
  @keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.7; }
  }

  /* Voice Agent Status */
  #voiceStatus {
    font-size: 12px;
    color: #888;
    margin-top: 8px;
    text-align: center;
  }
  #voiceStatus.active { color: #10b981; }
  #voiceStatus.error { color: #ef4444; }
</style>
</head>
<body>

<!-- Survey Container -->
<div id="survey-container"></div>

<!-- Main Content (hidden until survey is completed) -->
<div id="main-content">
<div id="stage">
  <div id="backdrop"></div>
  <video id="v0" playsinline muted preload="auto" loop crossorigin="anonymous"></video>
  <video id="v1" playsinline muted preload="auto" loop crossorigin="anonymous"></video>
  <div id="blackBox"></div>
</div>

<div id="controls">
  <button id="voiceAgentBtn">ðŸŽ¤ Talk to Sad Robot</button>
  <button id="endInteractionBtn" style="background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%); border-color: #ef4444; font-weight: 600;">End Interaction</button>
  <div id="voiceStatus">  </div>
</div>
</div> <!-- End main-content -->

<script type="module">
// Import dependencies
import { Conversation } from 'https://cdn.jsdelivr.net/npm/@11labs/client/+esm';
import { VideoMixer, intro_video, idle_video } from './js/videoMixer.js';
import EmotionAnalyzer from './js/emotionAnalyzer.js';
import './js/emotionDebug.js'; // Debug utilities
import './js/survey.js';

// Robot tag to identify which version of the robot (sad/happy) this session is using
// Change this to "happy" for the control group branch
// This should match the ROBOT_TAG in survey.js
const ROBOT_TAG = 'sad';

// Helper function to reset survey (for testing)
// Usage in console: resetSurvey()
window.resetSurvey = function() {
  localStorage.removeItem('consentCompleted');
  localStorage.removeItem('consentData');
  localStorage.removeItem('surveyCompleted');
  localStorage.removeItem('surveyCompletionTime');
  location.reload();
};

// Initialize video mixer and emotion analyzer only when main content is shown
let videoMixer = null;
let emotionAnalyzer = null;
let videoMixerInitialized = false;

function initializeVideoMixer() {
  if (videoMixerInitialized) return;

  const stage = document.getElementById('stage');
  if (!stage) return;

  videoMixer = new VideoMixer(stage);
  window.videoMixer = videoMixer; // Make available globally for other modules

  // Initialize emotion analyzer
  emotionAnalyzer = new EmotionAnalyzer();
  window.emotionAnalyzer = emotionAnalyzer; // Make available globally

  // Initialize but don't autoplay
  // videoMixer.initPlaylistBuffers();
  
  // Preload all videos that will be used
  // Get the playlists (they return arrays with {src: "..."} objects)
  // TODO: We can clean this up.
  const listeningPlaylist = videoMixer.activePlaylist(); // Get current playlist (listening by default)
  videoMixer._isTalking = true; // Temporarily switch to get talking playlist
  const talkingPlaylist = videoMixer.activePlaylist();
  videoMixer._isTalking = false; // Switch back
  
  // Extract just the src strings
  const listeningSrcs = listeningPlaylist.map(item => item.src);
  const talkingSrcs = talkingPlaylist.map(item => item.src);
  
  // Interweave the two playlists for optimal loading
  const maxLength = Math.max(talkingSrcs.length, listeningSrcs.length);
  const interweavedVideos = [];
  for (let i = 0; i < maxLength; i++) {
    if (i < talkingSrcs.length) interweavedVideos.push(talkingSrcs[i]);
    if (i < listeningSrcs.length) interweavedVideos.push(listeningSrcs[i]);
  }
  
  // Add intro and idle videos at the beginning
  const allVideos = [intro_video, idle_video, ...interweavedVideos];
  
  console.log('ðŸ“¦ Preloading', allVideos.length, 'videos in interweaved order...');
  
  // Preload each video
  let preloadedCount = 0;
  allVideos.forEach((videoSrc) => {
    const vid = document.createElement('video');
    vid.preload = 'auto';
    vid.muted = true;
    vid.src = videoSrc;
    vid.style.display = 'none';
    
    vid.addEventListener('canplaythrough', () => {
      preloadedCount++;
      console.log(`âœ“ Preloaded (${preloadedCount}/${allVideos.length}):`, videoSrc);
      
      if (preloadedCount === allVideos.length) {
        console.log('âœ“ All videos preloaded!');
      }
    }, { once: true });
    
    vid.addEventListener('error', (e) => {
      console.warn('âš ï¸ Failed to preload:', videoSrc, e);
    });
    
    document.body.appendChild(vid);
  });
  
  videoMixerInitialized = true;

  console.log('[Main] Video mixer and emotion analyzer initialized');
}

// Initialize video mixer when survey is completed or if already completed
window.addEventListener('surveyCompleted', () => {
  initializeVideoMixer();
});

// Check if consent and survey are already completed on page load
if (localStorage.getItem('consentCompleted') === 'true' && localStorage.getItem('surveyCompleted') === 'true') {
  // Consent and survey already completed, show main content and initialize video mixer
  const mainContent = document.getElementById('main-content');
  if (mainContent) {
    mainContent.style.display = 'block';
  }
  const surveyContainer = document.getElementById('survey-container');
  if (surveyContainer) {
    surveyContainer.style.display = 'none';
  }
  // Initialize video mixer after a short delay to ensure DOM is ready
  setTimeout(() => {
    initializeVideoMixer();
  }, 100);
}

// --- Voice Agent Integration ---
(() => {
  const voiceBtn = document.getElementById("voiceAgentBtn");
  const voiceStatus = document.getElementById("voiceStatus");
  let conversation = null;
  let isConnected = false;
  let isIdle = true;

  // Update status message
  function setStatus(message, type = "info") {
    voiceStatus.textContent = message;
    voiceStatus.className = type;
  }

  // Initialize ElevenLabs conversation
  async function initializeVoiceAgent() {
    try {
      setStatus("Connecting to Sad Robot...", "info");
      voiceBtn.classList.add("active");
      voiceBtn.disabled = true;

      // Get signed URL from our backend
      const response = await fetch("/api/get-signed-url");
      if (!response.ok) {
        throw new Error("Failed to get signed URL");
      }

      const { signedUrl, conversationId: backendConversationId } = await response.json();
      console.log("Backend provided conversation ID:", backendConversationId);

      setStatus("Starting conversation...", "info");

      // Initialize ElevenLabs conversation using the imported Conversation class
      conversation = await Conversation.startSession({
        signedUrl: signedUrl,
        onConnect: async () => {
          console.log("Voice agent connected");
          isConnected = true;
          window.conversation = conversation; // Expose globally for end interaction button

          voiceBtn.classList.remove("active");
          voiceBtn.classList.add("connected");
          voiceBtn.textContent = "ðŸ”´ End Conversation";
          voiceBtn.disabled = false;
          setStatus("Connected! Start talking to the Sad Robot", "active");

          // Capture and store the conversation ID after connection
          // Use setTimeout to ensure conversation object is fully initialized
          setTimeout(async () => {
            // Debug: Log all properties of the conversation object
            console.log("Conversation object keys:", Object.keys(conversation));
            console.log("Full conversation object:", conversation);

            // Check connection object
            if (conversation.connection) {
              console.log("Connection object keys:", Object.keys(conversation.connection));
              console.log("Connection object:", conversation.connection);
            }

            // Check options object (might contain conversation metadata)
            if (conversation.options) {
              console.log("Options object:", conversation.options);
            }

            // Try to extract conversation ID from multiple sources
            let conversationId = null;

            // Method 0: Use conversation ID from backend if available
            if (backendConversationId) {
              conversationId = backendConversationId;
              console.log("Using conversation ID from backend:", conversationId);
            }

            // Method 1: Check conversation object properties
            if (!conversationId) {
              conversationId =
                conversation.conversationId ||
                conversation.id ||
                conversation._id ||
                conversation.conversation_id ||
                conversation.sessionId ||
                conversation.session_id ||
                null;
            }

            // Method 2: Check connection object
            if (!conversationId && conversation.connection) {
              conversationId =
                conversation.connection.conversationId ||
                conversation.connection.id ||
                conversation.connection.conversation_id ||
                conversation.connection.agentId ||
                null;
            }

            // Method 3: Extract from signed URL (if available in options)
            if (!conversationId && conversation.options && conversation.options.signedUrl) {
              try {
                const url = new URL(conversation.options.signedUrl);
                conversationId = url.searchParams.get('conversation_id') ||
                                url.searchParams.get('conversationId') ||
                                url.pathname.split('/').pop() || // Last part of path
                                null;
                console.log("Attempting to extract from URL:", url.href);
              } catch (e) {
                console.log("Could not parse signed URL");
              }
            }

            console.log("Final extracted Conversation ID:", conversationId);

            if (conversationId) {
              // Get or create session ID
              let sessionId = localStorage.getItem('userSessionId');
              if (!sessionId) {
                sessionId = `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
                localStorage.setItem('userSessionId', sessionId);
              }

              // Store conversation ID in Supabase
              try {
                const response = await fetch('/api/update-conversation-id', {
                  method: 'POST',
                  headers: {
                    'Content-Type': 'application/json',
                  },
                  body: JSON.stringify({ sessionId, conversationId, robotTag: ROBOT_TAG }),
                });

                const result = await response.json();

                if (response.ok) {
                  console.log('âœ… Conversation ID stored successfully:', result);
                } else {
                  console.error('âŒ Failed to store conversation ID:', result);
                }
              } catch (error) {
                console.error('âŒ Error storing conversation ID:', error);
              }
            } else {
              console.warn('âš ï¸ No conversation ID found in Conversation object');
              console.warn('Available properties:', Object.keys(conversation));
              console.warn('ðŸ’¡ You may need to retrieve the conversation ID from the ElevenLabs API after the conversation ends');
            }
          }, 500);
        },
        onDisconnect: () => {
          console.log("Voice agent disconnected");
          isConnected = false;
          window.conversation = null; // Clear global reference
          voiceBtn.classList.remove("active", "connected");
          voiceBtn.textContent = "ðŸŽ¤ Talk to Sad Robot";
          voiceBtn.disabled = false;
          setStatus("", "info");
          conversation = null;
        },
        onError: (error) => {
          console.error("Voice agent error:", error);
          setStatus("Connection error. Please try again.", "error");
          voiceBtn.classList.remove("active", "connected");
          voiceBtn.textContent = "ðŸŽ¤ Talk to Sad Robot";
          voiceBtn.disabled = false;
          isConnected = false;
          window.conversation = null; // Clear global reference
          conversation = null;
        },
        onMessage: (message) => {
          console.log("Agent message:", message);

          if(isIdle) {
            videoMixer.setTalking(true);
            isIdle = false;
          }

          // Only analyze robot's responses (AI messages), not user messages
          if (message && message.source === 'ai' && message.message && window.emotionAnalyzer) {
            const text = message.message;
            window.emotionAnalyzer.storeMessage(text);
          }
        },
        onModeChange: (change) => {
          console.log("Preloading. Conversation mode change:", change);

          if(isIdle) {
            return; // Ignore initial mode changes before first message
          }

          if(change.mode === 'listening') {

            videoMixer.setTalking(false);

            // While the user speaks, we analyze the ai emotion and do a potential switch
            console.log('-- Affect:', emotionAnalyzer.currentAffect);
            var affect  = emotionAnalyzer.currentAffect;
            var mappedIndex = Math.round(affect); 

            if(mappedIndex > videoMixer.currentIndex()){
              videoMixer.queueNewCurrentIndex(mappedIndex);
            }

          } else if(change.mode === 'speaking') {
            videoMixer.setTalking(true);
          } else {
            console.log("Unknown conversation mode: " + change.mode, "error");
          }
        }
      });
    } catch (error) {
      console.error("Failed to initialize voice agent:", error);
      setStatus(`Failed to connect: ${error.message}`, "error");
      voiceBtn.classList.remove("active", "connected");
      voiceBtn.textContent = "ðŸŽ¤ Talk to Sad Robot";
      voiceBtn.disabled = false;
      isConnected = false;
      conversation = null;
    }
  }

  // End conversation
  async function endConversation() {
    if (conversation) {
      try {
        await conversation.endSession();
        conversation = null;
        window.conversation = null; // Clear global reference
        isConnected = false;
        voiceBtn.classList.remove("active", "connected");
        voiceBtn.textContent = "ðŸŽ¤ Talk to Sad Robot";
        setStatus("", "info");
        
        // Hide all videos
        if (videoMixer && videoMixer.vids) {
          videoMixer.vids.forEach(vid => {
            try {
              vid.pause();
              vid.classList.remove('active', 'on-top');
            } catch (e) {
              console.warn('Error hiding video:', e);
            }
          });
        }
      } catch (error) {
        console.error("Error ending conversation:", error);
      }
    }
  }
  
  // Expose endConversation globally for end interaction button
  window.endVoiceConversation = endConversation;

  // Start button click handler
  voiceBtn.addEventListener("click", async () => {
    if (isConnected) {
      await endConversation();
      return;
    }

    // Prevent double clicks
    voiceBtn.disabled = true;
    voiceBtn.classList.add('active');
    setStatus('Preparing...', 'info');

    try {
      // Initialize voice agent after a 5 second delay; do NOT await so the UI flow isn't blocked by the voice init.
      setTimeout(() => {
        console.log('Starting voice agent initialization during intro video...');
        initializeVoiceAgent().catch((err) => {
          console.error('initializeVoiceAgent failed:', err);
        });
      }, 5000);

      // Play intro video, then immediately transition to looping idle video
      // The idle video is prepped while intro plays, eliminating the black gap
      setStatus('Playing intro video...', 'info');
      videoMixer.playIntroThenIdle(intro_video, idle_video).catch((e) => { 
        console.warn('playIntroThenIdle failed', e); 
      });
      console.log(`[${new Date().toISOString()}] Intro->Idle sequence started.`);

      // Start the usual playlist mechanism
      // try { videoMixer.beginPlaylist(); } catch (e) { console.warn('beginPlaylist failed', e); }

    } catch (err) {
      console.error('Error during talk-button flow:', err);
      setStatus('Error starting conversation', 'error');
    } finally {
      voiceBtn.disabled = false;
      voiceBtn.classList.remove('active');
    }
  });

  // Cleanup on page unload
  window.addEventListener("beforeunload", () => {
    if (conversation) {
      conversation.endSession();
    }
  });
})();

// --- Audio Playback Monitor ---
(() => {
  let audioIsPlaying = false;
  let currentPlayingAudio = null;

  // Track all audio elements on the page
  function monitorAudioElement(audioElement) {
    audioElement.addEventListener('play', () => {
      audioIsPlaying = true;
      currentPlayingAudio = audioElement;
      console.log('ðŸ”Š Audio started playing');

      // Call custom callback if defined
      if (window.onAudioPlaybackChange) {
        window.onAudioPlaybackChange(true);
      }
    });

    audioElement.addEventListener('pause', () => {
      audioIsPlaying = false;
      currentPlayingAudio = null;
      console.log('ðŸ”‡ Audio paused');

      if (window.onAudioPlaybackChange) {
        window.onAudioPlaybackChange(false);
      }
    });

    audioElement.addEventListener('ended', () => {
      audioIsPlaying = false;
      currentPlayingAudio = null;
      console.log('âœ“ Audio ended');

      if (window.onAudioPlaybackChange) {
        window.onAudioPlaybackChange(false);
      }
    });
  }

  // Monitor existing audio elements
  document.querySelectorAll('audio').forEach(monitorAudioElement);

  // Watch for dynamically created audio elements (like from ElevenLabs)
  const observer = new MutationObserver((mutations) => {
    mutations.forEach((mutation) => {
      mutation.addedNodes.forEach((node) => {
        // Debug: log all node types being added
        console.log('ðŸ” Node added:', node.nodeName, node);

        if (node.tagName === 'AUDIO') {
          console.log('ðŸŽµ New audio element detected');
          monitorAudioElement(node);
        }
        // Also check child nodes
        if (node.querySelectorAll) {
          const audioElements = node.querySelectorAll('audio');
          if (audioElements.length > 0) {
            console.log('ðŸŽµ Found', audioElements.length, 'audio elements in child nodes');
            audioElements.forEach(monitorAudioElement);
          }
        }
      });
    });
  });

  // Start observing the document for audio elements
  observer.observe(document.body, {
    childList: true,
    subtree: true
  });

  // Periodically check for audio elements (in case they're in shadow DOM)
  setInterval(() => {
    const allAudio = document.querySelectorAll('audio');
    console.log('ðŸ” Periodic check: found', allAudio.length, 'audio elements');
    allAudio.forEach(audio => {
      if (!audio.dataset.monitored) {
        console.log('ðŸŽµ Found unmonitored audio element!');
        audio.dataset.monitored = 'true';
        monitorAudioElement(audio);
      }
    });
  }, 2000);

  // Public API
  window.audioMonitor = {
    isPlaying: () => audioIsPlaying,
    getCurrentAudio: () => currentPlayingAudio,
  };

  console.log('[Audio Monitor] Initialized - tracking all audio playback');
})();

// --- UI wiring ---
// This is just debugging stuff.
const $ = (s) => document.querySelector(s);

// Fullscreen toggle
const fullscreenBtn = $("#fullscreenBtn");

/*
fullscreenBtn.onclick = () => {
  if (!document.fullscreenElement && !document.webkitFullscreenElement && !document.mozFullScreenElement) {
    if (stage.requestFullscreen) {
      stage.requestFullscreen();
    } else if (stage.webkitRequestFullscreen) {
      stage.webkitRequestFullscreen();
    } else if (stage.mozRequestFullScreen) {
      stage.mozRequestFullScreen();
    }
  } else {
    if (document.exitFullscreen) {
      document.exitFullscreen();
    } else if (document.webkitExitFullscreen) {
      document.webkitExitFullscreen();
    } else if (document.mozCancelFullScreen) {
      document.mozCancelFullScreen();
    }
  }
};

// Update button text based on fullscreen state
const updateFullscreenButton = () => {
  if (document.fullscreenElement || document.webkitFullscreenElement || document.mozFullScreenElement) {
    fullscreenBtn.textContent = "â›¶ Exit Fullscreen";
  } else {
    fullscreenBtn.textContent = "â›¶ Fullscreen";
  }
};

document.addEventListener("fullscreenchange", updateFullscreenButton);
document.addEventListener("webkitfullscreenchange", updateFullscreenButton);
document.addEventListener("mozfullscreenchange", updateFullscreenButton);
*/

// End Interaction button - triggers post-study survey
const endInteractionBtn = $("#endInteractionBtn");
endInteractionBtn.onclick = () => {
  // Confirm before ending interaction
  if (confirm('Are you sure you want to end the interaction? You will be asked to complete a post-study survey.')) {
    // End voice conversation if active
    if (window.endVoiceConversation) {
      window.endVoiceConversation();
    }
    
    // Start post-survey
    if (window.startPostSurvey) {
      window.startPostSurvey();
    }
  }
};
</script>
</body>
</html>